{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Project Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Haokai Mao\n",
    "- Zhilin Kong\n",
    "- Kai Yeung\n",
    "- Haixin Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents and how they are measured\n",
    "- what you will be doing with the data\n",
    "- how performance/success will be measured\n",
    "\n",
    "\n",
    "**Our goal is to predict whether a patient is likely to have diabetes , prediabetes or be healthy. We are using the data set from the kaggle website. The data set was built by 441,445 individuals and has features. These features are either questions directly asked of participants, or calculated variables based on individual participant responses. The participants are all anonymous and keep their privacy.We will use SVM to perform the classification tasks(i.e., detecting diabetes). We first begin feature selection to reduce the dimensionality of the data, which can improve the performance of the SVM. Then, we split the data to train the SVM model. Next, we evaluate the performance of the trained SVM on the testing set using metrics such as accuracy, precision, recall, and F1-score. Finally, we optimize the SVM by doing a randomized search.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "\n",
    "Diabetes is among the most prevalent chronic diseases in the United States, impacting millions of Americans each year and exerting a significant financial burden on the economy. Diabetes is a serious chronic disease in which individuals lose the ability to effectively regulate levels of glucose in the blood, and can lead to reduced quality of life and life expectancy. After different foods are broken down into sugars during digestion, the sugars are then released into the bloodstream. This signals the pancreas to release insulin. Insulin helps enable cells within the body to use those sugars in the bloodstream for energy. Diabetes is generally characterized by either the body not making enough insulin or being unable to use the insulin that is made as effectively as needed.(CDC)*[[^CDC]](https://www.cdc.gov/diabetes/basics/diabetes.html)*\n",
    "\n",
    "Complications like heart disease, vision loss, lower-limb amputation, and kidney disease are associated with chronically high levels of sugar remaining in the bloodstream for those with diabetes. While there is no cure for diabetes, strategies like losing weight, eating healthily, being active, and receiving medical treatments can mitigate the harms of this disease in many patients. Early diagnosis can lead to lifestyle changes and more effective treatment, making predictive models for diabetes risk important tools for public and public health officials.(CDC)*[[^CDC]](https://www.cdc.gov/diabetes/managing/index.html)*\n",
    "\n",
    "\n",
    "We would like to classify people who have diabetes or prediabetes or are healthy. There are 1 in diabetics and 8 in prediabetes who do not know they have problems. Type II diabetes is the most common form. Diabetes will bring much financial impact to America."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once).\n",
    "\n",
    "**Diabetes Prediction: In this project, the goal is to predict whether a patient is likely to develop diabetes or not. Our goal contains three states: no diabetes, prediabetes, and diabetes. The input data includes various features such as age, BMI, blood pressure, and glucose level. An SVM model will be trained based on this data and then perform a multiclassification on patients.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "You should have a strong idea of what dataset(s) will be used to accomplish this project. \n",
    "\n",
    "If you know what (some) of the data you will use, please give the following information for each dataset:\n",
    "- link: https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset\n",
    "- Number of variables: 22\n",
    "- Number of observations: 507360\n",
    "- An observation consists of:\n",
    "  \n",
    "  'Diabetes' : 0 = no diabetes, 1 = prediabetes, 2 = diabetes; \n",
    "  \n",
    "  'HighBP' : 0 = no high, BP 1 = high BP;\n",
    "  \n",
    "  'HighChol' : 0 = no high cholesterol, 1 = high cholesterol;\n",
    "  \n",
    "  'CholCheck' : 0 = no cholesterol check in 5 years, 1 = yes cholesterol check in 5 years; \n",
    "  \n",
    "  'BMI' :Body Mass Index;\n",
    "  \n",
    "  'Smoker' : Have you smoked at least 100 cigarettes in your entire life? 0 = no 1 = yes;\n",
    "  \n",
    "  'Stroke' : (Ever told) you had a stroke. 0 = no 1 = yes;\n",
    "  \n",
    "  'HeartDiseaseorAttack' : coronary heart disease (CHD) or myocardial infarction (MI) 0 = no 1 = yes; \n",
    "  \n",
    "  'PhysActivity' : physical activity in past 30 days(not including job). 0 = no 1 = yes; \n",
    "  \n",
    "  'Fruits' : Consume Fruit 1 or more times per day 0 = no 1 = yes; \n",
    "  \n",
    "  'Veggies' : Consume Vegetables 1 or more times per day 0 = no 1 = yes;\n",
    "  \n",
    "  'HvyAlcoholConsump' :Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week) 0 = no 1 = yes; \n",
    "  \n",
    "  'AnyHealthcare' : ;Have any kind of health care coverage, including health insurance, prepaid plans such as HMO, etc. 0 = no 1 = yes;\n",
    "  \n",
    "  'NoDocbcCost' : Was there a time in the past 12 months when you needed to see a doctor but could not because of cost? 0 = no 1 = yes;\n",
    "  \n",
    "  'GenHlth' : Would you say that in general your health is: scale 1-5 1 = excellent 2 = very good 3 = good 4 = fair 5 = poor;\n",
    "  \n",
    "  'MentHlth' : Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good? scale 1-30 days;\n",
    "  \n",
    "  'PhysHlth' : Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good? scale 1-30 days; \n",
    "  \n",
    "  'DiffWalk' : Do you have serious difficulty walking or climbing stairs? 0 = no 1 = yes;\n",
    "  \n",
    "  'Sex': 0 = female 1 = male;\n",
    "  \n",
    "  'Age' : 13-level age category (_AGEG5YR see codebook) 1 = 18-24 9 = 60-64 13 = 80 or older; \n",
    "  \n",
    "  'Education' : Education level (EDUCA see codebook) scale 1-6 1 = Never attended school or only kindergarten 2 = Grades 1 through 8 (Elementary) 3 = Grades 9 through 11 (Some high school) 4 = Grade 12 or GED (High school graduate) 5 = College 1 year to 3 years (Some college or technical school) 6 = College 4 years or more (College graduate);\n",
    "  \n",
    "  'Income' : Income scale (INCOME2 see codebook) scale 1-8: 1 = less than 10,000 dollars, 5 = less than 35,000 dollars, 8 = 75,000 dollars or more.\n",
    "\n",
    "\n",
    "- what some critical variables are, how they are represented\n",
    "  \n",
    "  **For representations of variables, please see the above explanations. Critical variables: 'Diabetes', 'HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'GenHlth', 'DiffWalk', 'Sex', 'Age'.** \n",
    "\n",
    "\n",
    "- any special handling, transformations, cleaning, etc will be needed\n",
    "  \n",
    "  **We will drop the uncritical variables to reduce the dimensionality of the SVM model and hence improve efficiency and performance.** \n",
    "  \n",
    "If you don't yet know what your dataset(s) will be, you should describe what you desire in terms of the above bullets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Why might your solution work? Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. \n",
    "\n",
    "---\n",
    "\n",
    "### Solution:\n",
    "\n",
    "We decide to use three different algorithms to train 3 IID models, SVM, Logistic Regression and KNN. Then apply majority vote ensemble to output the final result.\n",
    "\n",
    "#### Data Preprocessing:\n",
    "\n",
    "1. Merge two datasets\n",
    "2. Remove invalid values\n",
    "3. Split into training and testing by 8:2 ratio. In this step we will use \"Train_test_split\"\n",
    "\n",
    "#### Training the SVM Model:\n",
    "\n",
    "1. We create SVM classifier and use grid search with cross-validation to identify the best hyperparameters.  \n",
    "   In this step we will use \"Pipeline\", \"GridSearchCV\" and \"SVC\".\n",
    "2. Choose one kernel function. We will try several kernel functions but probably radial basis function will be the best choice.  \n",
    "   The reason is it works well in high dimensional space, and our data is not linearly separable since our variables are mostly binary.\n",
    "3. Train the SVM model on training dataset.\n",
    "\n",
    "#### Training the Logistic Regression Model:\n",
    "\n",
    "1. We use grid search to find the best hyperparameter 'C'. \n",
    "2. To make sure it does not exceed the max iteration, we set max iter = 1000.\n",
    "3. Train the LR model on the training dataset.\n",
    "\n",
    "#### Training the KNN Model:\n",
    "\n",
    "1. We also use grid search to find the best hyperparameter \"n_neightbors\"\n",
    "2. Train the KNN model on the training dataset.\n",
    "\n",
    "#### We apply majority vote ensemble to output the final result\n",
    "\n",
    "1. Get the output from three different models, we output the result that at least two model agrees.\n",
    "\n",
    "For the packages, we will use sklearn:\n",
    "- Train_test_split\n",
    "- Pipeline\n",
    "- GridSearchCV\n",
    "- SVC\n",
    "- KNeighborsClassifier\n",
    "- LogisticRegression\n",
    "- Warning (ignore future warnings to provide a better look)\n",
    "\n",
    "#### Why our solution might work:\n",
    "\n",
    "SVM can handle large feature spaces and non-linear decision boundaries. Also, by using cross-validation, we can reduce the chance of overfitting to ensure the model can generalize well. \n",
    "Also, Logistic Regression and KNN are also good classifiers, we combine three of them to get the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms).\n",
    "\n",
    "### AUC-ROC\n",
    "#### Main Idea\n",
    "- We will use roc_auc_score in sklearn to evaluate the model.\n",
    "- To calculate the AUC-ROC score, we first calculate the TPR and FPR of the testing data, we will build an confusion matrix for those rates.\n",
    "- Then we can use sns to plot the curve, then we cancalculate the area under the ROC curve. \n",
    "\n",
    "#### Increase Sensitivity\n",
    "- Since we are using this model to detact a sick, then false negative is expensive. We want to increase the model's sensitivity.\n",
    "- We can do it by adjust the decision threshold, we can lower the threshold to classify more unsure cases as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "NEW SECTION!\n",
    "\n",
    "Please show any preliminary results you have managed to obtain.\n",
    "\n",
    "Examples would include:\n",
    "- Analyzing the suitability of a dataset or alogrithm for prediction/solving your problem \n",
    "- Performing feature selection or hand-designing features from the raw data. Describe the features available/created and/or show the code for selection/creation\n",
    "- Showing the performance of a base model/hyper-parameter setting.  Solve the task with one \"default\" algorithm and characterize the performance level of that base model.\n",
    "- Learning curves or validation curves for a particular model\n",
    "- Tables/graphs showing the performance of different models/hyper-parameters\n",
    "\n",
    "### Some Information for Kai Yeung & Haixin Chen:\n",
    "#### We currently uses three different algo to train three diff models. SVM, KNN and Logisitic regression. U can find the accuracy of each of them below. Maybe use them to calculate the majority vote emsemble accuracy if you need. Check our new proposed solution for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination. Get creative!\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "**Data Collection**\n",
    "\n",
    "  - To ensure that the data we use is unbiased and completely private, we only use datasets provided by an open-source resource (Kaggle). There is no personal information involved or terms of use issues with the data, the dataset is built by anonymous individuals. The data does not involve any trade secrets, and they are merely some physical variables.\n",
    "\n",
    "  - However, when using datasets that contain sensitive personal information, such as medical data on patients, we need to be careful to avoid infringing on the privacy of others. It is important to ensure that there are no personal identifiers in these datasets to avoid the disclosure of sensitive personal information. Furthermore, we must verify that open-source datasets are collected ethically and in compliance with relevant privacy regulations, including obtaining informed consent from individuals involved.\n",
    "\n",
    "**Data Storage**\n",
    "\n",
    "  - In order to protect the privacy of patients, it's important to ensure that the data is stored securely, using encryption and access controls to prevent unauthorized access. In addition to technical measures, it's important to establish ethical guidelines for the use of the data, such as obtaining informed consent from patients and ensuring that the data is used only for its intended purpose. This can help to prevent the misuse or unauthorized sharing of the data.\n",
    "\n",
    "**Data Analysis and Modeling**\n",
    "\n",
    "  - If the dataset used to train the SVM algorithm is not representative of the wider population, this can lead to biased results that may unfairly disadvantage certain groups. During the analysis, we will use graphs, such as scatter plots, to see if there exist some outliers that cannot be reasonably explained. By doing so, we can carefully select and preprocess the data to minimize the risk of bias and to carefully evaluate the performance of the model to ensure that it does not exhibit any unfair or harmful biases. Hence, we should regularly examine the data to identify any potential biases in advance and correct them in a timely manner to avoid any bias. Finally, it's important to communicate the results of the analysis clearly and transparently, providing insights that are both accurate and understandable to audiences. \n",
    "\n",
    "**Project or Model Deployment**\n",
    "\n",
    "  - We need to be aware that there is a class imbalance in the dataset, which may lead to biased and inaccurate machine-learning models. Given the imbalance in the dataset, when interpreting our results, we need to ensure that the machine learning model is transparent and explainable as well as use appropriate metrics to evaluate the performance of the machine learning model. This will allow us to identify and address any potential biases.\n",
    "\n",
    "**Reproducibility and replicability**\n",
    "\n",
    "  - Reproducibility and replicability are important considerations from both an ethical and technical perspective. To ensure reproducibility and replicability in this project, it's important to document the methods used for preprocessing the data, training the model, and evaluating its performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc...\n",
    "\n",
    "- The team members should respect each other\n",
    "- Work is clearly assigned to ensure that the work is done efficiently and effectively\n",
    "- Good communication and effective planning\n",
    "- Plan regular team meetings to keep the project on track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this with something meaningful that is appropriate for your needs. It doesn't have to be something that fits this format.  It doesn't have to be set in stone... \"no battle plan survives contact with the enemy\". But you need a battle plan nonetheless, and you need to keep it updated so you understand what you are trying to accomplish, who's responsible for what, and what the expected due dates are for each item.\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/17  |  5 PM |  Brainstorm topics/questions (all)；Search for datasets (Zhilin，Haokai)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 2/19  |  5 PM |  Discuss and assign tasks to group members (all) | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/22  | 5 PM  | Edit, finalize, and submit proposal （all） | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/24  | 5 PM  | Import & Wrangle Data | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 2/28  | 5 PM  | Finalize wrangling/EDA; Begin programming for project | Discuss/edit project code; Complete project |\n",
    "| 3/13  | 5 PM  | Complete analysis; Draft results/conclusion/discussion | Discuss/edit full project |\n",
    "| 3/19  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages & Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ImportPackages\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#DataCleaning\n",
    "diabetes012 = pd.read_csv(\"data/Diabetes012.csv\")\n",
    "diabetesBinary = pd.read_csv(\"data/DiabetesBinary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Diabetes_012', 'HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker',\n",
      "       'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies',\n",
      "       'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth',\n",
      "       'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education',\n",
      "       'Income'],\n",
      "      dtype='object')\n",
      "Index(['Diabetes_binary', 'HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker',\n",
      "       'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies',\n",
      "       'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth',\n",
      "       'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education',\n",
      "       'Income'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(diabetes012.columns)\n",
    "print(diabetesBinary.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge two dataframes\n",
    "diabetesBinaryRenamed = diabetesBinary.rename(columns={\"Diabetes_binary\": \"Diabetes\"})\n",
    "diabetes012Renamed = diabetes012.rename(columns={\"Diabetes_012\": \"Diabetes\"})\n",
    "diabetes = pd.concat([diabetesBinaryRenamed,diabetes012Renamed])\n",
    "diabetes = diabetes.dropna()\n",
    "diabetes[\"Diabetes\"] = diabetes[\"Diabetes\"].replace(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data to training and testing (8:2 ratio)\n",
    "trainDiabetes, testDiabetes = train_test_split(diabetes, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    345639\n",
       "1.0     60249\n",
       "Name: Diabetes, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDiabetes[\"Diabetes\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>...</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Diabetes  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
       "0       0.0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
       "1       0.0     0.0       0.0        0.0  25.0     1.0     0.0   \n",
       "2       0.0     1.0       1.0        1.0  28.0     0.0     0.0   \n",
       "3       0.0     1.0       0.0        1.0  27.0     0.0     0.0   \n",
       "4       0.0     1.0       1.0        1.0  24.0     0.0     0.0   \n",
       "\n",
       "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  AnyHealthcare  \\\n",
       "0                   0.0           0.0     0.0  ...            1.0   \n",
       "1                   0.0           1.0     0.0  ...            0.0   \n",
       "2                   0.0           0.0     1.0  ...            1.0   \n",
       "3                   0.0           1.0     1.0  ...            1.0   \n",
       "4                   0.0           1.0     1.0  ...            1.0   \n",
       "\n",
       "   NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  Education  \\\n",
       "0          0.0      5.0      18.0      15.0       1.0  0.0   9.0        4.0   \n",
       "1          1.0      3.0       0.0       0.0       0.0  0.0   7.0        6.0   \n",
       "2          1.0      5.0      30.0      30.0       1.0  0.0   9.0        4.0   \n",
       "3          0.0      2.0       0.0       0.0       0.0  0.0  11.0        3.0   \n",
       "4          0.0      2.0       3.0       0.0       0.0  0.0  11.0        5.0   \n",
       "\n",
       "   Income  \n",
       "0     3.0  \n",
       "1     1.0  \n",
       "2     8.0  \n",
       "3     6.0  \n",
       "4     4.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(507360, 23)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#diabetes = diabetes.drop_duplicates()\n",
    "diabetes = diabetes.reset_index()\n",
    "diabetes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>...</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "      <td>507360.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>126839.500000</td>\n",
       "      <td>0.148461</td>\n",
       "      <td>0.429001</td>\n",
       "      <td>0.424121</td>\n",
       "      <td>0.962670</td>\n",
       "      <td>28.382364</td>\n",
       "      <td>0.443169</td>\n",
       "      <td>0.040571</td>\n",
       "      <td>0.094186</td>\n",
       "      <td>0.756544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951053</td>\n",
       "      <td>0.084177</td>\n",
       "      <td>2.511392</td>\n",
       "      <td>3.184772</td>\n",
       "      <td>4.242081</td>\n",
       "      <td>0.168224</td>\n",
       "      <td>0.440342</td>\n",
       "      <td>8.032119</td>\n",
       "      <td>5.050434</td>\n",
       "      <td>6.053875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>73231.180312</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.494934</td>\n",
       "      <td>0.494209</td>\n",
       "      <td>0.189571</td>\n",
       "      <td>6.608688</td>\n",
       "      <td>0.496760</td>\n",
       "      <td>0.197294</td>\n",
       "      <td>0.292087</td>\n",
       "      <td>0.429169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215758</td>\n",
       "      <td>0.277653</td>\n",
       "      <td>1.068476</td>\n",
       "      <td>7.412839</td>\n",
       "      <td>8.717943</td>\n",
       "      <td>0.374065</td>\n",
       "      <td>0.496429</td>\n",
       "      <td>3.054217</td>\n",
       "      <td>0.985773</td>\n",
       "      <td>2.071146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>63419.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>126839.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>190259.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>253679.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               index       Diabetes         HighBP       HighChol  \\\n",
       "count  507360.000000  507360.000000  507360.000000  507360.000000   \n",
       "mean   126839.500000       0.148461       0.429001       0.424121   \n",
       "std     73231.180312       0.355556       0.494934       0.494209   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%     63419.750000       0.000000       0.000000       0.000000   \n",
       "50%    126839.500000       0.000000       0.000000       0.000000   \n",
       "75%    190259.250000       0.000000       1.000000       1.000000   \n",
       "max    253679.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "           CholCheck            BMI         Smoker         Stroke  \\\n",
       "count  507360.000000  507360.000000  507360.000000  507360.000000   \n",
       "mean        0.962670      28.382364       0.443169       0.040571   \n",
       "std         0.189571       6.608688       0.496760       0.197294   \n",
       "min         0.000000      12.000000       0.000000       0.000000   \n",
       "25%         1.000000      24.000000       0.000000       0.000000   \n",
       "50%         1.000000      27.000000       0.000000       0.000000   \n",
       "75%         1.000000      31.000000       1.000000       0.000000   \n",
       "max         1.000000      98.000000       1.000000       1.000000   \n",
       "\n",
       "       HeartDiseaseorAttack   PhysActivity  ...  AnyHealthcare    NoDocbcCost  \\\n",
       "count         507360.000000  507360.000000  ...  507360.000000  507360.000000   \n",
       "mean               0.094186       0.756544  ...       0.951053       0.084177   \n",
       "std                0.292087       0.429169  ...       0.215758       0.277653   \n",
       "min                0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%                0.000000       1.000000  ...       1.000000       0.000000   \n",
       "50%                0.000000       1.000000  ...       1.000000       0.000000   \n",
       "75%                0.000000       1.000000  ...       1.000000       0.000000   \n",
       "max                1.000000       1.000000  ...       1.000000       1.000000   \n",
       "\n",
       "             GenHlth       MentHlth       PhysHlth       DiffWalk  \\\n",
       "count  507360.000000  507360.000000  507360.000000  507360.000000   \n",
       "mean        2.511392       3.184772       4.242081       0.168224   \n",
       "std         1.068476       7.412839       8.717943       0.374065   \n",
       "min         1.000000       0.000000       0.000000       0.000000   \n",
       "25%         2.000000       0.000000       0.000000       0.000000   \n",
       "50%         2.000000       0.000000       0.000000       0.000000   \n",
       "75%         3.000000       2.000000       3.000000       0.000000   \n",
       "max         5.000000      30.000000      30.000000       1.000000   \n",
       "\n",
       "                 Sex            Age      Education         Income  \n",
       "count  507360.000000  507360.000000  507360.000000  507360.000000  \n",
       "mean        0.440342       8.032119       5.050434       6.053875  \n",
       "std         0.496429       3.054217       0.985773       2.071146  \n",
       "min         0.000000       1.000000       1.000000       1.000000  \n",
       "25%         0.000000       6.000000       4.000000       5.000000  \n",
       "50%         0.000000       8.000000       5.000000       7.000000  \n",
       "75%         1.000000      10.000000       6.000000       8.000000  \n",
       "max         1.000000      13.000000       6.000000       8.000000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journey Begins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First use partial dataset to test our model\n",
    "diabetes = diabetes.sample(random_state = 888, n = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target variable (y)\n",
    "X = diabetes.drop('Diabetes', axis=1)\n",
    "y = diabetes['Diabetes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = SVC()\n",
    "\n",
    "# # Create a pipeline\n",
    "# pipe = Pipeline([\n",
    "#     ('scaler', scaler),\n",
    "#     ('selector', selector),\n",
    "#     ('clf', clf)\n",
    "# ])\n",
    "\n",
    "# # Define the hyperparameter grid to search\n",
    "# param_grid = {\n",
    "#     'selector__k': [5, 8, 10],\n",
    "#     'clf__C': [0.1, 1, 10],\n",
    "#     'clf__kernel': ['linear', 'poly', 'rbf'],\n",
    "#     'clf__gamma': ['scale', 'auto']\n",
    "# }\n",
    "\n",
    "# # Use cross-validation to find the best hyperparameters\n",
    "# scorer = make_scorer(roc_auc_score)\n",
    "# # grid_search = GridSearchCV(pipe, param_grid, cv=10, scoring=scorer,verbose=3)\n",
    "# # grid_search.fit(X_train, y_train)\n",
    "# model = SVC(C = 100, kernel = \"linear\")\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8455\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "modelLogistic = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "# Use grid search\n",
    "gridSearchParameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(modelLogistic,gridSearchParameters,cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "y_pred_logistic = grid.predict(X_test)\n",
    "\n",
    "accuracyLogistic = grid.score(X_test, y_test)\n",
    "print(\"accuracy\",accuracyLogistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.852\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "param_grid = {'n_neighbors': [3, 5, 7, 9]}\n",
    "modelKnn = KNeighborsClassifier()\n",
    "\n",
    "gridKnn = GridSearchCV(modelKnn,param_grid,cv = 5)\n",
    "gridKnn.fit(X_train,y_train)\n",
    "\n",
    "y_pred_knn = gridKnn.predict(X_test)\n",
    "accuracyKnn = gridKnn.score(X_test, y_test)\n",
    "\n",
    "print(\"accuracy\",accuracyKnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Majority Vote Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions on the test set\n",
    "# y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# # Calculate the evaluation metrics\n",
    "# acc = accuracy_score(y_test, y_pred)\n",
    "# prec = precision_score(y_test, y_pred)\n",
    "# rec = recall_score(y_test, y_pred)\n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "# roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# # Print the evaluation metrics\n",
    "# print(f\"Accuracy: {acc:.4f}\")\n",
    "# print(f\"Precision: {prec:.4f}\")\n",
    "# print(f\"Recall: {rec:.4f}\")\n",
    "# print(f\"F1-score: {f1:.4f}\")\n",
    "# print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "# print(f\"Best hyperparameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
